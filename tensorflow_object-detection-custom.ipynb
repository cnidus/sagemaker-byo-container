{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setup\n",
    "To start things off, we'll set the s3 bucket name, download the training data to S3 and upload the customized training container to Elastic Container Registry (ECR).\n",
    "\n",
    "If you don't have an S3 bucket to use, please go set one up now and note down the bucket name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clint in /usr/local/lib/python2.7/site-packages\n",
      "Requirement already satisfied: args in /usr/local/lib/python2.7/site-packages (from clint)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 9.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "sudo pip install clint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set basic parameters\n",
    "Setup the environment with required modules. You will need to __Change the bucket name__ below to the one you created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 260 ms, sys: 120 ms, total: 380 ms\n",
      "Wall time: 448 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sys\n",
    "sys.path.append('/home/ec2-user/anaconda3/lib/python3.6/site-packages/')\n",
    "\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket='cnidus-ml-iad' # customize to your bucket\n",
    "\n",
    "#Set your target \n",
    "#containers = {'us-west-2': '107995894928.dkr.ecr.us-west-2.amazonaws.com/object-detection'}\n",
    "containers = {'us-east-1': '366895301435.dkr.ecr.us-west-2.amazonaws.com/object-detection'}\n",
    "training_image = containers[boto3.Session().region_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload training data to S3\n",
    "Next step is to download a public training dataset, format it appropriately for our model and upload it to S3.\n",
    "\n",
    "For this example, we're using the [\"pets\" dataset](http://www.robots.ox.ac.uk/~vgg/data/pets/) from Oxford University."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#From tensorflow/models/research/\n",
    "wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
    "wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
    "tar -xvf annotations.tar.gz\n",
    "tar -xvf images.tar.gz\n",
    "python object_detection/dataset_tools/create_pet_tf_record.py \\\n",
    "    --label_map_path=object_detection/data/pet_label_map.pbtxt \\\n",
    "    --data_dir=`pwd` \\\n",
    "    --output_dir=`pwd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/pet_label_map.pbtxt\n",
      "pet_label_map.pbtxt\n",
      "Downloading: https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/dataset_tools/create_pet_tf_record.py\n",
      "create_pet_tf_record.py\n",
      "Finished downloading training dataset files\n"
     ]
    }
   ],
   "source": [
    "#from clint.textui import progress\n",
    "import requests\n",
    "\n",
    "URLList = [\n",
    "#    'http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz',\n",
    "#    'http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz',\n",
    "    'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/pet_label_map.pbtxt',\n",
    "    'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/dataset_tools/create_pet_tf_record.py'  \n",
    "]\n",
    "\n",
    "#Download each file and show a progress bar\n",
    "for URL in URLList:\n",
    "    print(\"Downloading: \" + str(URL))\n",
    "    fname = URL.split(\"/\")[-1] #url.split(\"/\")[-2:]\n",
    "    print(fname)\n",
    "    r = requests.get(URL, stream=True)\n",
    "    with open(fname, 'wb') as f:\n",
    "#        print(r.headers.get('content-length'))\n",
    "#        total_length = int(r.headers.get('content-length'))\n",
    "#        for chunk in progress.bar(r.iter_content(chunk_size=1024), expected_size=(total_length/1024) + 1): \n",
    "#            if chunk:\n",
    "                f.write(r.content)\n",
    "#                f.flush()\n",
    "\n",
    "print(\"Finished downloading training dataset files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the customized container to ECR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For this training, we will run it for 10 minutes so as to have a demo of it.\n",
    "max_run_time = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training using Amazon sagemaker CreateTrainingJob API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "# create unique job name \n",
    "job_name_prefix = 'object-detection-notebook'\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "job_name = job_name_prefix + timestamp\n",
    "training_params = \\\n",
    "{\n",
    "    # specify the training docker image\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": training_image,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": 's3://{}/{}/output'.format(bucket, job_name_prefix)\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.p3.2xlarge\",\n",
    "        \"VolumeSizeInGB\": 50\n",
    "    },\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": {\n",
    "        \"max_run_time\": str(max_run_time) # after this time training job will terminate itself\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 20*60 # 20 minutes. After this sagemaker will stop training\n",
    "    },\n",
    "#Training data should be inside a subdirectory called \"train\"\n",
    "#Validation data should be inside a subdirectory called \"validation\"\n",
    "#The algorithm currently only supports fullyreplicated model (where data is copied onto each machine)\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"training\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": 's3://{}/pet_detection_data/tf_record'.format(bucket),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "#             \"ContentType\": \"application/x-recordio\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "print('Training job name: {}'.format(job_name))\n",
    "print('\\nInput Data Location: {}'.format(training_params['InputDataConfig'][0]['DataSource']['S3DataSource']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the Amazon SageMaker training job\n",
    "sagemaker = boto3.client(service_name='sagemaker')\n",
    "sagemaker.create_training_job(**training_params)\n",
    "\n",
    "# confirm that the training job has started\n",
    "status = sagemaker.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print('Training job current status: {}'.format(status))\n",
    "\n",
    "try:\n",
    "    # wait for the job to finish and report the ending status\n",
    "    sagemaker.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=job_name)\n",
    "    training_info = sagemaker.describe_training_job(TrainingJobName=job_name)\n",
    "    status = training_info['TrainingJobStatus']\n",
    "    print(\"Training job ended with status: \" + status)\n",
    "except:\n",
    "    print('Training failed to start')\n",
    "     # if exception is raised, that means it has failed\n",
    "    message = sagemaker.describe_training_job(TrainingJobName=job_name)['FailureReason']\n",
    "    print('Training failed with the following error: {}'.format(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_info = sagemaker.describe_training_job(TrainingJobName=job_name)\n",
    "status = training_info['TrainingJobStatus']\n",
    "print(\"Training job ended with status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
