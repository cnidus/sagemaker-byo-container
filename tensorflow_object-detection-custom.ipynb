{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setup\n",
    "To start things off, we'll set the s3 bucket name, download the training data to S3 and upload the customized training container to Elastic Container Registry (ECR).\n",
    "\n",
    "If you don't have an S3 bucket to use, please go set one up now and note down the bucket name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set basic parameters\n",
    "Setup the environment with required modules. You will need to __Change the bucket name__ below to the one you created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import sys\n",
    "sys.path.append('/home/ec2-user/anaconda3/lib/python3.6/site-packages/')\n",
    "\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket='cnidus-ml-iad' # customize to your bucket\n",
    "\n",
    "#Set your target \n",
    "#containers = {'us-west-2': '107995894928.dkr.ecr.us-west-2.amazonaws.com/object-detection'}\n",
    "containers = {'us-east-1': '366895301435.dkr.ecr.us-west-2.amazonaws.com/object-detection'}\n",
    "training_image = containers[boto3.Session().region_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload training data to S3\n",
    "Next step is to download a public training dataset, format it appropriately for our model and upload it to S3.\n",
    "\n",
    "For this example, we're using the [\"pets\" dataset](http://www.robots.ox.ac.uk/~vgg/data/pets/) from Oxford University."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the training sets and dataset tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "data_dir = \"./data/\"\n",
    "tools_dir = \"./object_detection/\"\n",
    "\n",
    "URLList = [\n",
    "#    {'src': 'http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz','dst': data_dir},\n",
    "#    {'src': 'http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz', 'dst': data_dir},\n",
    "    {'src': 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/pet_label_map.pbtxt', 'dst': tools_dir},\n",
    "    {'src': 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/dataset_tools/create_pet_tf_record.py', 'dst': './'},\n",
    "    {'src': 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/utils/dataset_util.py', 'dst': str(tools_dir + '/utils/')},\n",
    "    {'src': 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/utils/label_map_util.py', 'dst': str(tools_dir + '/utils/')},\n",
    "    {'src': 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/protos/string_int_label_map.proto', 'dst': str(tools_dir + '/protos/')}\n",
    "]\n",
    "\n",
    "#Download each file\n",
    "for URL in URLList:\n",
    "    #Create the dst directory if it doesnt exist\n",
    "    if not os.path.exists(URL['dst']):\n",
    "        os.makedirs(URL['dst'])\n",
    "    fname = URL['dst'] + URL['src'].split(\"/\")[-1]\n",
    "    print(\"Downloading: \" + str(URL['src']) + \" to: \" + fname )\n",
    "    r = requests.get(URL['src'], stream=True)\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "print('\\n' + 'Finished downloading training dataset files')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the training set and reformat as TFRecord format\n",
    "The Tensorflow Object Detection API expects data to be in the TFRecord format, so we'll now run the create_pet_tf_record script to convert from the raw Oxford-IIIT Pet dataset into TFRecords.\n",
    "\n",
    "First, let's extract the training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfr_dir = './tfrecord/'\n",
    "\n",
    "import tarfile\n",
    "fileList = [\n",
    "    './data/images.tar.gz',\n",
    "    './data/annotations.tar.gz'\n",
    "    ]\n",
    "\n",
    "for file in fileList:\n",
    "    print('Extracting: ' + file)\n",
    "    tar = tarfile.open(file)\n",
    "    tar.extractall(data_dir)\n",
    "    tar.close()\n",
    "print('\\n'+ 'Done!')\n",
    "\n",
    "#Create the output directory if it doesnt exist\n",
    "if not os.path.exists(tfr_dir):\n",
    "    os.makedirs(tfr_dir)\n",
    "\n",
    "#\n",
    "sys.path.append('/home/ec2-user/anaconda3/lib/python3.6/site-packages/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the conversion script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# From tensorflow/models/research/\n",
    "python3 ./create_pet_tf_record.py \\\n",
    "    --label_map_path=./object_detection/pet_label_map.pbtxt \\\n",
    "    --data_dir=./data/ \\\n",
    "    --output_dir=./tfrecord/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the customized container to ECR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For this training, we will run it for 10 minutes so as to have a demo of it.\n",
    "max_run_time = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training using Amazon sagemaker CreateTrainingJob API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "# create unique job name \n",
    "job_name_prefix = 'object-detection-notebook'\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "job_name = job_name_prefix + timestamp\n",
    "training_params = \\\n",
    "{\n",
    "    # specify the training docker image\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": training_image,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": 's3://{}/{}/output'.format(bucket, job_name_prefix)\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.p3.2xlarge\",\n",
    "        \"VolumeSizeInGB\": 50\n",
    "    },\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": {\n",
    "        \"max_run_time\": str(max_run_time) # after this time training job will terminate itself\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 20*60 # 20 minutes. After this sagemaker will stop training\n",
    "    },\n",
    "#Training data should be inside a subdirectory called \"train\"\n",
    "#Validation data should be inside a subdirectory called \"validation\"\n",
    "#The algorithm currently only supports fullyreplicated model (where data is copied onto each machine)\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"training\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": 's3://{}/pet_detection_data/tf_record'.format(bucket),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "#             \"ContentType\": \"application/x-recordio\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "print('Training job name: {}'.format(job_name))\n",
    "print('\\nInput Data Location: {}'.format(training_params['InputDataConfig'][0]['DataSource']['S3DataSource']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the Amazon SageMaker training job\n",
    "sagemaker = boto3.client(service_name='sagemaker')\n",
    "sagemaker.create_training_job(**training_params)\n",
    "\n",
    "# confirm that the training job has started\n",
    "status = sagemaker.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print('Training job current status: {}'.format(status))\n",
    "\n",
    "try:\n",
    "    # wait for the job to finish and report the ending status\n",
    "    sagemaker.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=job_name)\n",
    "    training_info = sagemaker.describe_training_job(TrainingJobName=job_name)\n",
    "    status = training_info['TrainingJobStatus']\n",
    "    print(\"Training job ended with status: \" + status)\n",
    "except:\n",
    "    print('Training failed to start')\n",
    "     # if exception is raised, that means it has failed\n",
    "    message = sagemaker.describe_training_job(TrainingJobName=job_name)['FailureReason']\n",
    "    print('Training failed with the following error: {}'.format(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_info = sagemaker.describe_training_job(TrainingJobName=job_name)\n",
    "status = training_info['TrainingJobStatus']\n",
    "print(\"Training job ended with status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
